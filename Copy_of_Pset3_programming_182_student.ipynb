{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Pset3_programming_182_student.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CrKGJ-GK82yM",
        "W3BGbWOu179M"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veridie/first-contributions/blob/master/Copy_of_Pset3_programming_182_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrKGJ-GK82yM"
      },
      "source": [
        "# Overall Setup [no changes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2"
      },
      "source": [
        "Install dependancies and functions for displaying videos\n",
        "\n",
        "Setup code source: StarAI\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ"
      },
      "source": [
        "#CELL 1\n",
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI"
      },
      "source": [
        "#CELL 2\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[box2d] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "#CELL 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "#CELL 4\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "#CELL 5\n",
        "\n",
        "def show_video(folder):\n",
        "  mp4list = glob.glob(folder+'/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env, folder):\n",
        "  env = Monitor(env, './'+folder, force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M"
      },
      "source": [
        "# Lunar Lander Setup! [no changes, but look at output]\n",
        "For more information about the domain, visit\n",
        "https://gym.openai.com/envs/LunarLander-v2/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen"
      },
      "source": [
        "#CELL 6\n",
        "folder = 'video-test'\n",
        "#create the lunar lander environment\n",
        "env = wrap_env(gym.make(\"LunarLander-v2\"),folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89"
      },
      "source": [
        "#CELL 7\n",
        "#check out the lunar lander action space! \n",
        "#There are 8 states as defined in the GitHub, 6 of which are continuous and the final 2 are binary\n",
        "print(env.observation_space.shape[0])\n",
        "#There are 4 actions (fire the main engine, left engine, right engine, or do nothing)\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nj5sjsk15IT"
      },
      "source": [
        "#CELL 8\n",
        "#This is the baseline code to be able to implement the RL algorithms!\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "    env.render()\n",
        "\n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample()  #this is a random agent\n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "    \n",
        "    if done: \n",
        "      break;        \n",
        "env.close()\n",
        "show_video(folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSsv0RnwHtZK"
      },
      "source": [
        "# Q-Learning Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZjtObyYHyWl"
      },
      "source": [
        "## Reward\n",
        "## States / set up \n",
        "\n",
        "#CELL 9\n",
        "#create the lunar lander environment\n",
        "\n",
        "\"\"\"\n",
        "TODO: Discretizing the continuous observation state space. The final two values\n",
        "    are binary and therefore only have 2 values (0 or 1) but the other values \n",
        "    can be separated into any number of buckets (greater than 3).\n",
        "    According to OpenAI \"the useful range is -1 .. +1, but spikes can be higher\"\n",
        "\"\"\"\n",
        "\n",
        "bucket_num = [3,3,3,3,3,3,2,2] #some default values! Set your own. \n",
        "\n",
        "def get_Q_state(state):\n",
        "    \"\"\"\n",
        "    OPTIONAL: change the bucketing method!\n",
        "    This method takes the state (a length 8 numpy array) from the environment\n",
        "    and returns a string (the state string)encoding the discrete states \n",
        "    (formatted with dashes between values). \n",
        "    \"\"\"\n",
        "    assert(state.shape[0] == 8)\n",
        "\n",
        "    bucket = [0. for index in range(8)]\n",
        "    \n",
        "    for index in range(8):\n",
        "        if (index == 6 or index == 7):\n",
        "            bucket[index] = int(state[index])\n",
        "        else:\n",
        "            #two big buckets for the extremes (<-2 or >2)\n",
        "            if (state[index] <= -2):\n",
        "                bucket[index] = 0\n",
        "            elif (state[index] >= 2):\n",
        "                bucket[index] = bucket_num[index]-1\n",
        "            else: #several uniform buckets for the other indexes\n",
        "                bucket[index] = int((state[index]+2)*bucket_num[index]//4)\n",
        "    \n",
        "    bucket_string = \"\"\n",
        "    for index in range(8):\n",
        "        bucket_string = bucket_string + str(bucket[index]) + \"-\"\n",
        "    #returns a Q-bucket state that looks like 1-2-1-2-2-1-0-0_action\n",
        "    return bucket_string[:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvfYuNnhJQID"
      },
      "source": [
        "#CELL 10\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import pickle\n",
        "\n",
        "folder = 'video-final'\n",
        "env = wrap_env(gym.make(\"LunarLander-v2\"),folder)\n",
        "\n",
        "\"\"\"\n",
        "TODO: Writing the Q-Learning Algorithm to Train the Agent.\n",
        "    1. Write the body of the Q-learning algorithm\n",
        "    2. Tune the hyper-parameters and the values in bucket_num and have fun! \n",
        "\"\"\"\n",
        "\n",
        "q_table = {}\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "episodes = 1\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = [] #store the number of epochs per episode\n",
        "all_rewards = [] #store the penalties per episode\n",
        "\n",
        "\n",
        "for i in range(0, episodes):\n",
        "    state = env.reset()\n",
        "    print(state) \n",
        "    epochs, total_reward = 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        \"\"\"\n",
        "        You will find this code particularly helpful to get the next state from \n",
        "        your chosen action:\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        \"\"\"\n",
        "        \n",
        "    if i % 50 == 0:\n",
        "        clear_output(wait=True)\n",
        "    print(f\"Episode: {i},\\t Epochs: {epochs},\\tReward: {total_reward}\")\n",
        "    \n",
        "    all_epochs.append(epochs)\n",
        "    all_rewards.append(total_reward)\n",
        "\n",
        "    if i % 1000 == 0: #adjust to save the q_table dictionary at checkpoints\n",
        "        pickle.dump( q_table, open( \"q_table.p\", \"wb\" ) )\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qORUR9hgJUBw"
      },
      "source": [
        "#CELL 11\n",
        "\"\"\"\n",
        "Some sample code to help you evaluate agent's performance after Q-learning!\n",
        "Adjust accordingly to generate some nice metrics. \n",
        "This code will not be graded.\n",
        "\"\"\"\n",
        "folder = 'video-eval'\n",
        "env = wrap_env(gym.make(\"LunarLander-v2\"),folder)\n",
        "q_table = pickle.load( open( \"q_table.p\", \"rb\" ) )\n",
        "\n",
        "total_epochs, total_reward = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, epoch_reward = 0, 0\n",
        "\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        state_string = get_Q_state(state)\n",
        "        action_val, action = get_action(state_string)\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        epochs += 1\n",
        "        epoch_reward += reward\n",
        "\n",
        "    total_reward += epoch_reward\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average reward per episode: {total_reward / episodes}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9Qpt99fa_NX"
      },
      "source": [
        "#CELL 12\n",
        "\"\"\"\n",
        "TODO: Some sample code to help generate a video of a sample run! \n",
        "Adjust accordingly to generate some nice video files. \n",
        "This code will not be graded.\n",
        "\"\"\"\n",
        "folder = 'video-visual'\n",
        "env = wrap_env(gym.make(\"LunarLander-v2\"),folder)\n",
        "\n",
        "state = env.reset()\n",
        "\n",
        "while True:\n",
        "    env.render()\n",
        "\n",
        "    #your agent goes here\n",
        "    state_string = get_Q_state(state)\n",
        "    action_val, action = get_action(state_string)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    \n",
        "    if done: \n",
        "      break;        \n",
        "env.close()\n",
        "show_video(folder)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}